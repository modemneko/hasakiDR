import os
import asyncio
from datetime import datetime
from typing import TypedDict, List, Annotated, Dict, Any
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from google.generativeai import types as genai_types
from browser_use import Agent, Browser, BrowserConfig 

from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.staticfiles import StaticFiles
import uvicorn
import time


# proxy = "http://127.0.0.1:7890"
# os.environ["HTTP_PROXY"] = proxy


browser = Browser(
    BrowserConfig(
        headless=True,
        disable_security=True
    )
)


GOOGLE_API_KEY = ""
os.environ["GEMINI_API_KEY"] = GOOGLE_API_KEY

class ResearchState(TypedDict):
    initial_query: str
    current_task: str
    research_history: Annotated[List[BaseMessage], add_messages]
    accumulated_findings: str
    final_report: str
    max_iterations: int
    current_iteration: int


llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash",
        google_api_key=GOOGLE_API_KEY,
        safety_settings={
            genai_types.HarmCategory.HARM_CATEGORY_HARASSMENT: genai_types.HarmBlockThreshold.BLOCK_NONE,
            genai_types.HarmCategory.HARM_CATEGORY_HATE_SPEECH: genai_types.HarmBlockThreshold.BLOCK_NONE,
            genai_types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: genai_types.HarmBlockThreshold.BLOCK_NONE,
            genai_types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: genai_types.HarmBlockThreshold.BLOCK_NONE,
        },
        temperature=0.5,
    )
    
async def planner_node(state: ResearchState) -> Dict[str, Any]:
    print("\n--- Planner ---")
    current_iteration = state.get("current_iteration", 0)
    initial_query = state["initial_query"]
    accumulated_findings = state.get("accumulated_findings", "æ— åˆå§‹å‘ç°")
    research_history_messages = state.get("research_history", [])

    history_context_for_prompt = ""
    if current_iteration > 0:
        history_context_for_prompt = f"å½“å‰å·²ç§¯ç´¯çš„ç ”ç©¶å‘ç°æ¦‚è¦ï¼š\n{accumulated_findings}\n"

        if research_history_messages:
            last_ai_message_content = None
            for msg in reversed(research_history_messages):
                if isinstance(msg, AIMessage) and msg.content:
                     last_ai_message_content = msg.content
                     break

            if last_ai_message_content:
                 history_context_for_prompt += f"\næœ€è¿‘å®Œæˆçš„ç ”ç©¶æ­¥éª¤å›é¡¾ï¼š\n{last_ai_message_content[:500]}...\n"


    if current_iteration == 0:
        prompt = f"""
        ç ”ç©¶ä¸»é¢˜: "{initial_query}"

        è¯·ä¸ºç½‘é¡µæµè§ˆä»£ç† BrowserAgent ç”Ÿæˆä¸€ä¸ªä»»åŠ¡æŒ‡ä»¤ã€‚
        æŒ‡ä»¤åº”å¼•å¯¼å…¶å®Œæˆä»¥ä¸‹æ­¥éª¤ï¼š
        1. é’ˆå¯¹ä¸Šè¿°ç ”ç©¶ä¸»é¢˜è¿›è¡Œå…³é”®è¯æœç´¢ã€‚
        2. ä»æœç´¢ç»“æœä¸­è¯†åˆ«å¹¶é€‰æ‹©1-2ä¸ªæœ€ç›¸å…³çš„ç½‘é¡µé“¾æ¥ã€‚
        3. è®¿é—®è¿™äº›é“¾æ¥ï¼Œå¹¶ä»æ¯ä¸ªé¡µé¢ä¸­æå–ä¸ç ”ç©¶ä¸»é¢˜ç›´æ¥ç›¸å…³çš„æ ¸å¿ƒä¿¡æ¯ã€‚
        4. æ€»ç»“æ”¶é›†åˆ°çš„æ‰€æœ‰ä¿¡æ¯ã€‚

        ç›´æ¥è¾“å‡ºè¿™ä¸ªä»»åŠ¡æŒ‡ä»¤ã€‚
        """
    else:
        prompt = f"""
        æ€»ä½“ç ”ç©¶ä¸»é¢˜: "{initial_query}"

        {history_context_for_prompt}
        è¯·ä¸ºç½‘é¡µæµè§ˆä»£ç† BrowserAgent ç”Ÿæˆä¸‹ä¸€æ­¥çš„ç ”ç©¶ä»»åŠ¡æŒ‡ä»¤ã€‚
        è¯¥æŒ‡ä»¤åº”æ—¨åœ¨æ·±åŒ–å·²æœ‰å‘ç°ã€å¡«è¡¥ä¿¡æ¯ç©ºç™½æˆ–æ¢ç´¢æ–°è§’åº¦ã€‚
        æŒ‡ä»¤åº”å¼•å¯¼å…¶å®Œæˆç±»ä¼¼ä»¥ä¸‹æ­¥éª¤ï¼š
        1. æ ¹æ®éœ€è¦è¡¥å……æˆ–æ·±åŒ–çš„å…·ä½“ä¿¡æ¯ç‚¹ï¼Œè¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„æœç´¢æˆ–è®¿é—®å·²çŸ¥ç½‘ç«™ã€‚
        2. ä»ç»“æœä¸­é€‰æ‹©æœ€åˆé€‚çš„é“¾æ¥è®¿é—®ï¼ˆå¦‚æœè¿›è¡Œäº†æœç´¢ï¼‰ã€‚
        3. ä»ç›®æ ‡é¡µé¢æå–ä¸å½“å‰ç‰¹å®šç ”ç©¶ç„¦ç‚¹ç›¸å…³çš„è¯¦ç»†ä¿¡æ¯ã€‚
        4. æ€»ç»“æ–°æ”¶é›†åˆ°çš„ä¿¡æ¯ã€‚

        å¦‚æœåˆ†æè®¤ä¸ºå½“å‰ä¿¡æ¯å·²è¶³å¤Ÿå…¨é¢å›ç­”æ€»ä½“ç ”ç©¶ä¸»é¢˜ï¼Œè¯·ç›´æ¥è¾“å‡º "ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"ã€‚
        å¦åˆ™ï¼Œè¯·ç›´æ¥è¾“å‡ºç»™ BrowserAgent çš„ä¸‹ä¸€æ­¥ä»»åŠ¡æŒ‡ä»¤ã€‚
        """

    response = await llm.ainvoke(prompt)
    next_task_for_browser_use = response.content.strip()
    print(f"Planner generated task for Agent: {next_task_for_browser_use}")

    if "ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š" in next_task_for_browser_use:
        if accumulated_findings and len(accumulated_findings) > 50:
            print("Planner suggests generating final report.")
            return {"current_task": "FINAL_REPORT_TASK", "current_iteration": current_iteration + 1}
        else:
            print("Planner suggested report, but findings are insufficient. Forcing further research.")
            fallback_task = f"ç»§ç»­æ·±å…¥ç ”ç©¶ '{initial_query}' çš„æ ¸å¿ƒæ–¹é¢ï¼Œå¯»æ‰¾æ›´å…·ä½“çš„ç»†èŠ‚ã€ä¾‹å­æˆ–è¯æ®ã€‚"
            return {"current_task": fallback_task, "current_iteration": current_iteration + 1}


    return {
        "current_task": next_task_for_browser_use,
        "current_iteration": current_iteration + 1
    }


async def researcher_node(state: ResearchState) -> Dict[str, Any]:
    print("\n--- Researcher ---")
    task_to_research = state.get("current_task", "")
    if not task_to_research or task_to_research in ["FINAL_REPORT_TASK", "ERROR_LLM_UNAVAILABLE"]:
        print("æ— æ–°ç ”ç©¶ä»»åŠ¡ã€å‡†å¤‡ç”ŸæˆæŠ¥å‘Šæˆ–LLMé”™è¯¯ï¼Œè·³è¿‡ç½‘é¡µæœç´¢ã€‚")
        return {"research_history": [AIMessage(content=f"è·³è¿‡ç ”ç©¶èŠ‚ç‚¹ã€‚ä»»åŠ¡: {task_to_research}")]}

    if browser is None:
         print("é”™è¯¯: æµè§ˆå™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œ Researcher èŠ‚ç‚¹ã€‚")
         return {"research_history": [AIMessage(content=f"ç ”ç©¶ä»»åŠ¡ '{task_to_research}' æ‰§è¡Œå¤±è´¥: æµè§ˆå™¨æœªåˆå§‹åŒ–ã€‚")]}

    if llm is None:
        print("é”™è¯¯: LLM æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œ Researcher èŠ‚ç‚¹ã€‚")
        return {"research_history": [AIMessage(content=f"ç ”ç©¶ä»»åŠ¡ '{task_to_research}' æ‰§è¡Œå¤±è´¥: LLM æœªåˆå§‹åŒ–ã€‚")]}

    print(f"Researching: {task_to_research}")
    try:
        agent = Agent(
            browser=browser,
            task=task_to_research,
            llm=llm,
            use_vision=False,
            max_failures=10,
            retry_delay=5,
            )
        result_text = await agent.run()

        summary = str(result_text) if result_text else "æœªèƒ½è·å–æ˜ç¡®ä¿¡æ¯ã€‚"
        message_content = f"ç ”ç©¶ä»»åŠ¡: {task_to_research}\nç ”ç©¶ç»“æœ:\n{summary[:5000]}"
        return {"research_history": [AIMessage(content=message_content)]}
    except Exception as e:
        print(f"Error in researcher_node for task '{task_to_research}': {e}")
        return {"research_history": [AIMessage(content=f"ç ”ç©¶ä»»åŠ¡ '{task_to_research}' æ‰§è¡Œå¤±è´¥: {e}")]}


async def synthesizer_node(state: ResearchState) -> Dict[str, Any]:
    print("\n--- Synthesizer ---")
    initial_query = state["initial_query"]
    research_history_messages = state.get("research_history", [])
    # ä»å†å²æ¶ˆæ¯ä¸­è¿‡æ»¤å‡º AI çš„ç ”ç©¶ç»“æœ
    research_results = [msg.content for msg in research_history_messages if isinstance(msg, AIMessage) and "ç ”ç©¶ç»“æœ:" in msg.content]

    if not research_results:
         print("æ— æ–°çš„ç ”ç©¶ç»“æœå¯ä¾›åˆæˆã€‚")
         return {"accumulated_findings": state.get("accumulated_findings", "æ— ")}

    all_results_text = "\n\n".join(research_results)

    if llm is None:
         print("é”™è¯¯: LLM æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œ Synthesizer èŠ‚ç‚¹ã€‚")
         # è¿”å›å½“å‰ç§¯ç´¯çš„å‘ç°ï¼Œä¸è¿›è¡Œæ›´æ–°
         return {"accumulated_findings": state.get("accumulated_findings", "LLMä¸å¯ç”¨ï¼Œæœªèƒ½åˆæˆæ–°å‘ç°ã€‚")}

    prompt = f"""æ•´åˆä»¥ä¸‹å…³äº "{initial_query}" çš„ç ”ç©¶ä¿¡æ¯ï¼Œç”Ÿæˆæ›´æ–°çš„ç´¯ç§¯å‘ç°æ¦‚è¦ã€‚å¦‚æœå·²æœ‰ä¸€äº›ç´¯ç§¯å‘ç°ï¼Œè¯·åœ¨æ–°çš„å‘ç°åŸºç¡€ä¸Šæ›´æ–°å®ƒã€‚

    ç ”ç©¶ä¿¡æ¯ç‰‡æ®µ:
    {all_results_text}

    è¯·è¾“å‡ºç®€æ´ã€è¿è´¯çš„ç´¯ç§¯å‘ç°æ¦‚è¦ã€‚"""
    response = await llm.ainvoke(prompt)
    updated_findings = response.content.strip()
    print(f"Synthesized findings length: {len(updated_findings)}")
    return {"accumulated_findings": updated_findings}


async def final_report_node(state: ResearchState) -> Dict[str, Any]:
    print("\n--- Final Report Generator ---")
    initial_query = state["initial_query"]
    accumulated_findings = state.get("accumulated_findings", "æœªèƒ½ç§¯ç´¯å‘ç°ã€‚")

    if llm is None:
         print("é”™è¯¯: LLM æœªåˆå§‹åŒ–ï¼Œæ— æ³•ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šã€‚")
         return {"final_report": "é”™è¯¯: LLM æœªåˆå§‹åŒ–ï¼Œæœªèƒ½ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šã€‚\n" + accumulated_findings}


    history_summary_for_report = ""
    # ä»…åœ¨æœ‰è¶³å¤Ÿå†å²æ—¶åŒ…å«è¯¦ç»†æ­¥éª¤
    if state.get("research_history") and len(state["research_history"]) > 2:
        history_summary_for_report = "\n\nè¯¦ç»†ç ”ç©¶æ­¥éª¤å’Œå‘ç°æ¦‚è¦ï¼š\n"
        step_counter = 1
        for msg in state["research_history"]:
             if isinstance(msg, AIMessage) and msg.content and "ç ”ç©¶ä»»åŠ¡:" in msg.content:
                  # æå–ä»»åŠ¡å’Œç»“æœçš„æ¦‚è§ˆ
                  parts = msg.content.split("\nç ”ç©¶ç»“æœ:\n", 1)
                  task_summary = parts[0].replace("ç ”ç©¶ä»»åŠ¡:", "").strip()
                  result_preview = parts[1][:300] + "..." if len(parts) > 1 and parts[1] else "æ— æ˜ç¡®ç»“æœã€‚"
                  history_summary_for_report += f"\næ­¥éª¤ {step_counter}:\n ä»»åŠ¡: {task_summary}\n ç»“æœé¢„è§ˆ: {result_preview}\n"
                  step_counter += 1
        if step_counter == 1: # å¦‚æœæ²¡æœ‰æå–åˆ°æœ‰æ•ˆçš„ç ”ç©¶æ­¥éª¤
            history_summary_for_report = ""


    prompt = f"""ç ”ç©¶ä¸»é¢˜: "{initial_query}"

    æ ¸å¿ƒç´¯ç§¯å‘ç°:
    {accumulated_findings}
    {history_summary_for_report if history_summary_for_report else "æ— è¯¦ç»†ç ”ç©¶æ­¥éª¤å›é¡¾ã€‚"}

    è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œæ’°å†™ä¸€ä»½å…¨é¢ã€ç»“æ„æ¸…æ™°çš„ç ”ç©¶æŠ¥å‘Šã€‚"""
    response = await llm.ainvoke(prompt)
    report = response.content.strip()
    print(f"Final Report generated, length: {len(report)}")
    return {"final_report": report}

# --- 4. å®šä¹‰æ¡ä»¶è¾¹ ---

def should_continue(state: ResearchState) -> str:
    current_iteration = state.get("current_iteration", 0)
    max_iterations = state.get("max_iterations", 3)
    current_task = state.get("current_task", "")

    if current_task == "FINAL_REPORT_TASK":
        print("æ¡ä»¶åˆ¤æ–­: ä»»åŠ¡ä¸ºç”ŸæˆæŠ¥å‘Šï¼Œæµç¨‹è½¬å‘ final_report_generator")
        return "generate_report"
    if current_task == "ERROR_LLM_UNAVAILABLE":
         print("æ¡ä»¶åˆ¤æ–­: LLM ä¸å¯ç”¨ï¼Œæµç¨‹è½¬å‘ final_report_generator (ç”Ÿæˆé”™è¯¯æŠ¥å‘Š)")
         return "generate_report"
    if current_iteration >= max_iterations:
        print(f"æ¡ä»¶åˆ¤æ–­: è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({max_iterations})ï¼Œæµç¨‹è½¬å‘ final_report_generator")
        return "generate_report"

    print("æ¡ä»¶åˆ¤æ–­: ç»§ç»­ç ”ç©¶ï¼Œæµç¨‹è½¬å‘ planner")
    return "continue_research"



workflow = StateGraph(ResearchState)
workflow.add_node("planner", planner_node)
workflow.add_node("researcher", researcher_node)
workflow.add_node("synthesizer", synthesizer_node)
workflow.add_node("final_report_generator", final_report_node)

workflow.set_entry_point("planner")
workflow.add_edge("planner", "researcher")
workflow.add_edge("researcher", "synthesizer")
workflow.add_conditional_edges(
    "synthesizer",
    should_continue,
    {"continue_research": "planner", "generate_report": "final_report_generator"}
)
workflow.add_edge("final_report_generator", END)

app_graph = workflow.compile()

# Dictionary to store ongoing and completed research progress
research_progress = {}
# Dictionary to store completed reports for retrieval
completed_reports = {}

async def run_hasaki_research(initial_query: str, max_iterations: int = 3):
    global research_progress, completed_reports
    # Use a unique key for the query, maybe sanitize it or use a hash if queries can be very long/complex
    query_key = initial_query # Simple key for now

    # Initialize progress for the new query
    research_progress[query_key] = {
        "progress": "Starting...",
        "final_report": None,
        "logs": [], # Initialize logs list
        "token_usage": None, # Placeholder, actual token usage tracking would require more integration
        "synthesizer_info": None, # Placeholder for synthesizer summary
        "final_report_info": None, # Placeholder for final report summary
        "start_time": time.time(),
        "elapsed_time": 0,
        "is_complete": False, # Add a completion flag
        "error": None, # Add an error field
    }

    inputs = {
        "initial_query": initial_query,
        "max_iterations": max_iterations,
        "current_iteration": 0,
        "research_history": [],
        "accumulated_findings": "æ— åˆå§‹å‘ç°ã€‚",
        "final_report": "",
    }
    print(f"\nğŸš€ Starting hasaki research for: '{initial_query}' (max {max_iterations} iterations)")

    try:
        async for output in app_graph.astream(inputs):
            # Capture and log all intermediate outputs
            for key, value in output.items():
                if key != '__end__':
                    # Format log entry to include node name and a snippet of the value
                    log_entry_content = str(value)
                    if len(log_entry_content) > 500:
                         log_entry_content = log_entry_content[:500] + "..."
                    log_entry = f"[{key}] {log_entry_content}"
                    research_progress[query_key]["logs"].append(log_entry)
                    print(f"Stream output: {log_entry}") # Optional: print to console

            # Update progress based on the latest state
            # Get the latest state from the last output chunk
            last_output_key = list(output.keys())[-1]
            current_state = output.get('__end__') or output.get(last_output_key)

            if current_state:
                 if current_state.get('final_report'):
                    research_progress[query_key]["progress"] = "ç ”ç©¶å®Œæˆï¼Œç”ŸæˆæŠ¥å‘Šã€‚"
                    research_progress[query_key]["final_report"] = current_state['final_report']
                    research_progress[query_key]["final_report_info"] = f"Final Report generated, length: {len(current_state['final_report'])}"
                    research_progress[query_key]["is_complete"] = True # Mark as complete
                    # Store the completed report
                    completed_reports[query_key] = {
                        "query": initial_query,
                        "report": current_state['final_report'],
                        "timestamp": datetime.now().isoformat(),
                        "elapsed_time": time.time() - research_progress[query_key]["start_time"],
                    }
                 elif current_state.get('accumulated_findings'):
                    # Update progress with a summary of findings
                    research_progress[query_key]["progress"] = "ç ”ç©¶è¿›è¡Œä¸­... ç´¯ç§¯å‘ç°æ¦‚è¦: " + (current_state['accumulated_findings'][:200] + "..." if current_state['accumulated_findings'] else "æ— ")
                    research_progress[query_key]["synthesizer_info"] = f"Synthesized findings length: {len(current_state['accumulated_findings'])}"
                 elif current_state.get('current_task'):
                     task_preview = str(current_state['current_task'])
                     if len(task_preview) > 100:
                         task_preview = task_preview[:100] + "..."
                     research_progress[query_key]["progress"] = f"ç ”ç©¶è¿›è¡Œä¸­... å½“å‰ä»»åŠ¡: {task_preview}"
                 else:
                    research_progress[query_key]["progress"] = "ç ”ç©¶è¿›è¡Œä¸­..." # Default progress

            research_progress[query_key]["elapsed_time"] = time.time() - research_progress[query_key]["start_time"]
            # No need for asyncio.sleep(1) here, astream yields as it progresses

        # After the astream loop finishes, the final state should be available
        # The last output from astream should contain the final state if it reached END
        # A final check to ensure completion status and report are captured
        if not research_progress[query_key]["is_complete"]:
             # If astream finished but didn't mark as complete, it might have ended without reaching the final node
             print("\nâš ï¸ hasaki research finished stream but did not reach the final report node.")
             research_progress[query_key]["progress"] = "ç ”ç©¶å®Œæˆï¼Œä½†æœªèƒ½ç”ŸæˆæŠ¥å‘Šã€‚"
             research_progress[query_key]["is_complete"] = True # Mark as complete even if no report
             research_progress[query_key]["elapsed_time"] = time.time() - research_progress[query_key]["start_time"]


        # Return the final report or a status message
        return research_progress[query_key].get('final_report') if research_progress[query_key].get('final_report') else "ç ”ç©¶æœªèƒ½ç”ŸæˆæŠ¥å‘Šã€‚"


    except Exception as e:
        print(f"\nâŒ An error occurred during graph execution: {e}")
        research_progress[query_key]["progress"] = f"ç ”ç©¶æ‰§è¡ŒæœŸé—´å‘ç”Ÿé”™è¯¯: {e}"
        research_progress[query_key]["elapsed_time"] = time.time() - research_progress[query_key]["start_time"]
        research_progress[query_key]["is_complete"] = True # Mark as complete on error
        research_progress[query_key]["error"] = str(e)
        # Store error state in completed reports as well
        completed_reports[query_key] = {
            "query": initial_query,
            "report": f"ç ”ç©¶æ‰§è¡ŒæœŸé—´å‘ç”Ÿé”™è¯¯: {e}",
            "timestamp": datetime.now().isoformat(),
            "elapsed_time": time.time() - research_progress[query_key]["start_time"],
            "error": str(e),
        }
        return f"ç ”ç©¶æ‰§è¡ŒæœŸé—´å‘ç”Ÿé”™è¯¯: {e}"


# --- 7. æ„å»º FastAPI Web UI ---
app = FastAPI()

# Mount static files (like index.html, CSS, JS if separate)
app.mount("/static", StaticFiles(directory="e:/HakusAI"), name="static")


@app.get("/", response_class=HTMLResponse)
async def read_root():
    # Serve index.html from the static directory
    return FileResponse("e:/HakusAI/index.html")

@app.post("/research")
async def start_research(request: Request):
    global research_progress
    try:
        data = await request.json()
        query = data.get("query")
        max_iterations = data.get("max_iterations", 3)
        deep_research = data.get("deep_research", False)

        if not query:
            return {"report": "é”™è¯¯ï¼šæœªæä¾›ç ”ç©¶ä¸»é¢˜ã€‚"}

        print(f"Web UI Received Query: {query}, Max Iterations: {max_iterations}, Deep Research: {deep_research}")

        # Use the query itself as the key for progress tracking
        query_key = query

        # Prevent starting research if one is already running for this query
        if query_key in research_progress and not research_progress[query_key]["is_complete"] and research_progress[query_key]["error"] is None:
             return {"report": "è¯¥ç ”ç©¶ä¸»é¢˜å·²åœ¨è¿›è¡Œä¸­ã€‚"}

        if deep_research:
            # Run hasaki research in a background task
            asyncio.create_task(run_hasaki_research(query, max_iterations=max_iterations))
            return {"report": "hasakiç ”ç©¶å·²å¯åŠ¨ï¼Œè¯·ç¨åæŸ¥çœ‹ç ”ç©¶è¿›åº¦ã€‚"}
        else:
            # Run regular research (single pass or limited iterations)
            # For simplicity, let's make non-deep research just run the graph once or with minimal iterations
            # Or, we can make the graph itself handle the non-deep case based on max_iterations=1
            # Let's reuse run_hasaki_research but with max_iterations=1 for non-deep
            final_report = await run_hasaki_research(query, max_iterations=1) # Use max_iterations=1 for non-deep
            # For non-deep, we can return the report directly
            report_data = completed_reports.get(query_key, {})
            return {"report": report_data.get("report", final_report), "error": report_data.get("error")}


    except Exception as e:
        print(f"Error in /research endpoint: {e}")
        return {"report": f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {e}"}

@app.get("/research_progress")
async def get_research_progress(query: str):
    global research_progress
    # Return a copy to avoid external modification issues
    # Use the query itself as the key
    query_key = query
    progress_data = research_progress.get(query_key, {"report": "ç ”ç©¶å°šæœªå¯åŠ¨æˆ–æœªæ‰¾åˆ°ç ”ç©¶è¿›åº¦ã€‚", "final_report": None, "is_complete": True, "error": None, "logs": [], "elapsed_time": 0})
    return progress_data

@app.get("/completed_reports")
async def list_completed_reports():
    global completed_reports
    # Return a list of completed report summaries (query and timestamp)
    report_list = []
    for query, data in completed_reports.items():
        report_list.append({
            "query": data["query"],
            "timestamp": data["timestamp"],
            "elapsed_time": data["elapsed_time"],
            "has_error": "error" in data,
        })
    # Sort by timestamp, newest first
    report_list.sort(key=lambda x: x["timestamp"], reverse=True)
    return report_list

@app.get("/report/{query:path}") # Use path converter to handle queries with slashes
async def get_report(query: str):
    global completed_reports
    # Use the query itself as the key
    query_key = query
    report_data = completed_reports.get(query_key)
    if report_data:
        return {"query": report_data["query"], "report": report_data["report"], "timestamp": report_data["timestamp"], "elapsed_time": report_data["elapsed_time"], "error": report_data.get("error")}
    else:
        return {"report": "æœªæ‰¾åˆ°è¯¥ç ”ç©¶æŠ¥å‘Šã€‚", "error": "Report not found"}


if __name__ == "__main__":
    # Ensure the static directory exists (where index.html is)
    if not os.path.exists("e:/HakusAI"):
        print("Error: e:/HakusAI directory not found.")
        exit()

    uvicorn.run(app, host="127.0.0.1", port=7860)